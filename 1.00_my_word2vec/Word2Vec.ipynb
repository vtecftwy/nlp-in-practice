{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with Word2Vec in Gensim and making it work!\n",
    "\n",
    "The idea behind Word2Vec is pretty simple. We are making and assumption that you can tell the meaning of a word by the company it keeps. This is analogous to the saying *show me your friends, and I'll tell who you are*. So if you have two words that have very similar neighbors (i.e. the usage context is about the same), then these words are probably quite similar in meaning or are at least highly related. For example, the words `shocked`,`appalled` and `astonished` are typically used in a similar context. \n",
    "\n",
    "In this tutorial, you will learn how to use the Gensim implementation of Word2Vec and actually get it to work! I have heard a lot of complaints about poor performance etc, but its really a combination of two things, (1) your input data and (2) your parameter settings. Note that the training algorithms in this package were ported from the [original Word2Vec implementation by Google](https://arxiv.org/pdf/1301.3781.pdf) and extended with additional functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and logging\n",
    "\n",
    "First, we start with our imports and get logging established:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\nlp-2\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "# imports needed and set up logging\n",
    "import gzip\n",
    "import gensim \n",
    "import logging\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset \n",
    "Next, is our dataset. The secret to getting Word2Vec really working for you is to have lots and lots of text data. In this case I am going to use data from the [OpinRank](http://kavita-ganesan.com/entity-ranking-data/) dataset. This dataset has full user reviews of cars and hotels. I have specifically concatenated all of the hotel reviews into one big file which is about 97MB compressed and 229MB uncompressed. We will use the compressed file for this tutorial. Each line in this file represents a hotel review. You can download the OpinRank Word2Vec dataset here.\n",
    "\n",
    "To avoid confusion, while gensim’s word2vec tutorial says that you need to pass it a sequence of sentences as its input, you can always pass it a whole review as a sentence (i.e. a much larger size of text), and it should not make much of a difference. \n",
    "\n",
    "Now, let's take a closer look at this data below by printing the first line. You can see that this is a pretty hefty review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_folder = Path()\n",
    "raw_data_folder = w2v_folder / 'data-raw'\n",
    "models_folder = w2v_folder / 'models'\n",
    "path_to_data_file = raw_data_folder /'reviews_data.txt.gz'\n",
    "\n",
    "create_new_model = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_file=str(path_to_data_file.absolute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"Oct 12 2009 \\tNice trendy hotel location not too bad.\\tI stayed in this hotel for one night. As this is a fairly new place some of the taxi drivers did not know where it was and/or did not want to drive there. Once I have eventually arrived at the hotel, I was very pleasantly surprised with the decor of the lobby/ground floor area. It was very stylish and modern. I found the reception's staff geeting me with 'Aloha' a bit out of place, but I guess they are briefed to say that to keep up the coroporate image.As I have a Starwood Preferred Guest member, I was given a small gift upon-check in. It was only a couple of fridge magnets in a gift box, but nevertheless a nice gesture.My room was nice and roomy, there are tea and coffee facilities in each room and you get two complimentary bottles of water plus some toiletries by 'bliss'.The location is not great. It is at the last metro stop and you then need to take a taxi, but if you are not planning on going to see the historic sites in Beijing, then you will be ok.I chose to have some breakfast in the hotel, which was really tasty and there was a good selection of dishes. There are a couple of computers to use in the communal area, as well as a pool table. There is also a small swimming pool and a gym area.I would definitely stay in this hotel again, but only if I did not plan to travel to central Beijing, as it can take a long time. The location is ok if you plan to do a lot of shopping, as there is a big shopping centre just few minutes away from the hotel and there are plenty of eating options around, including restaurants that serve a dog meat!\\t\\r\\n\"\n"
     ]
    }
   ],
   "source": [
    "with gzip.open (path_to_data_file, 'rb') as f:\n",
    "# with open (data_file, 'rb') as f:\n",
    "    for i,line in enumerate (f):\n",
    "        print(line)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read files into a list\n",
    "Now that we've had a sneak peak of our dataset, we can read it into a list so that we can pass this on to the Word2Vec model. Notice in the code below, that I am directly reading the \n",
    "compressed file. I'm also doing a mild pre-processing of the reviews using `gensim.utils.simple_preprocess (line)`. This does some basic pre-processing such as tokenization, lowercasing, etc and returns back a list of tokens (words). Documentation of this pre-processing method can be found on the official [Gensim documentation site](https://radimrehurek.com/gensim/utils.html). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-11 10:45:38,356 : INFO : reading file D:\\PyProjects\\_references_nb\\10_ml-stats-applications\\nlp-00-nlp-in-practice\\0.01_word2vec\\data-raw\\reviews_data.txt.gz...this may take a while\n",
      "2019-05-11 10:45:38,358 : INFO : read 0 reviews\n",
      "2019-05-11 10:45:40,463 : INFO : read 10000 reviews\n",
      "2019-05-11 10:45:42,235 : INFO : read 20000 reviews\n",
      "2019-05-11 10:45:44,248 : INFO : read 30000 reviews\n",
      "2019-05-11 10:45:46,284 : INFO : read 40000 reviews\n",
      "2019-05-11 10:45:48,345 : INFO : read 50000 reviews\n",
      "2019-05-11 10:45:50,443 : INFO : read 60000 reviews\n",
      "2019-05-11 10:45:52,147 : INFO : read 70000 reviews\n",
      "2019-05-11 10:45:53,741 : INFO : read 80000 reviews\n",
      "2019-05-11 10:45:55,768 : INFO : read 90000 reviews\n",
      "2019-05-11 10:45:57,364 : INFO : read 100000 reviews\n",
      "2019-05-11 10:45:58,988 : INFO : read 110000 reviews\n",
      "2019-05-11 10:46:00,584 : INFO : read 120000 reviews\n",
      "2019-05-11 10:46:02,364 : INFO : read 130000 reviews\n",
      "2019-05-11 10:46:04,201 : INFO : read 140000 reviews\n",
      "2019-05-11 10:46:05,804 : INFO : read 150000 reviews\n",
      "2019-05-11 10:46:07,531 : INFO : read 160000 reviews\n",
      "2019-05-11 10:46:09,142 : INFO : read 170000 reviews\n",
      "2019-05-11 10:46:11,309 : INFO : read 180000 reviews\n",
      "2019-05-11 10:46:13,100 : INFO : read 190000 reviews\n",
      "2019-05-11 10:46:14,915 : INFO : read 200000 reviews\n",
      "2019-05-11 10:46:16,645 : INFO : read 210000 reviews\n",
      "2019-05-11 10:46:18,491 : INFO : read 220000 reviews\n",
      "2019-05-11 10:46:20,101 : INFO : read 230000 reviews\n",
      "2019-05-11 10:46:21,837 : INFO : read 240000 reviews\n",
      "2019-05-11 10:46:23,599 : INFO : read 250000 reviews\n",
      "2019-05-11 10:46:24,487 : INFO : Done reading data files\n"
     ]
    }
   ],
   "source": [
    "def read_input(input_file):\n",
    "    \"\"\"This method reads the input file which is in gzip format\"\"\"\n",
    "    \n",
    "    logging.info(f\"reading file {input_file}...this may take a while\")\n",
    "    \n",
    "    with gzip.open (input_file, 'rb') as f:\n",
    "        for i, line in enumerate (f): \n",
    "\n",
    "            if (i%10000==0):\n",
    "                logging.info (f\"read {i} reviews\")\n",
    "            # do some pre-processing and return a list of words for each review text\n",
    "            yield gensim.utils.simple_preprocess(line)\n",
    "\n",
    "\n",
    "if create_new_model:\n",
    "#   read the tokenized reviews into a list\n",
    "#   each review item becomes a series of words\n",
    "#   so this becomes a list of lists\n",
    "    documents = list (read_input (data_file))\n",
    "    logging.info(\"Done reading data files\")    \n",
    "else: \n",
    "    logging.info('Not reading text files as existing model will be loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Word2Vec model\n",
    "\n",
    "Training the model is fairly straightforward. You just instantiate Word2Vec and pass the reviews that we read in the previous step (the `documents`). So, we are essentially passing on a list of lists. Where each list within the main list contains a set of tokens from a user review. Word2Vec uses all these tokens to internally create a vocabulary. And by vocabulary, I mean a set of unique words.\n",
    "\n",
    "After building the vocabulary, we just need to call `train(...)` to start training the Word2Vec model. Training on the [OpinRank](http://kavita-ganesan.com/entity-ranking-data/) dataset takes about 10 minutes so please be patient while running your code on this dataset.\n",
    "\n",
    "Behind the scenes we are actually training a simple neural network with a single hidden layer. But, we are actually not going to use the neural network after training. Instead, the goal is to learn the weights of the hidden layer. These weights are essentially the word vectors that we’re trying to learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-11 10:48:12,839 : INFO : collecting all words and their counts\n",
      "2019-05-11 10:48:12,840 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-05-11 10:48:13,057 : INFO : PROGRESS: at sentence #10000, processed 1655714 words, keeping 25777 word types\n",
      "2019-05-11 10:48:13,270 : INFO : PROGRESS: at sentence #20000, processed 3317863 words, keeping 35016 word types\n",
      "2019-05-11 10:48:13,531 : INFO : PROGRESS: at sentence #30000, processed 5264072 words, keeping 47518 word types\n",
      "2019-05-11 10:48:13,797 : INFO : PROGRESS: at sentence #40000, processed 7081746 words, keeping 56675 word types\n",
      "2019-05-11 10:48:14,114 : INFO : PROGRESS: at sentence #50000, processed 9089491 words, keeping 63744 word types\n",
      "2019-05-11 10:48:14,377 : INFO : PROGRESS: at sentence #60000, processed 11013723 words, keeping 76781 word types\n",
      "2019-05-11 10:48:14,580 : INFO : PROGRESS: at sentence #70000, processed 12637525 words, keeping 83194 word types\n",
      "2019-05-11 10:48:14,765 : INFO : PROGRESS: at sentence #80000, processed 14099751 words, keeping 88454 word types\n",
      "2019-05-11 10:48:14,965 : INFO : PROGRESS: at sentence #90000, processed 15662149 words, keeping 93352 word types\n",
      "2019-05-11 10:48:15,164 : INFO : PROGRESS: at sentence #100000, processed 17164487 words, keeping 97881 word types\n",
      "2019-05-11 10:48:15,363 : INFO : PROGRESS: at sentence #110000, processed 18652292 words, keeping 102127 word types\n",
      "2019-05-11 10:48:15,547 : INFO : PROGRESS: at sentence #120000, processed 20152529 words, keeping 105918 word types\n",
      "2019-05-11 10:48:15,748 : INFO : PROGRESS: at sentence #130000, processed 21684330 words, keeping 110099 word types\n",
      "2019-05-11 10:48:15,957 : INFO : PROGRESS: at sentence #140000, processed 23330206 words, keeping 114103 word types\n",
      "2019-05-11 10:48:16,158 : INFO : PROGRESS: at sentence #150000, processed 24838754 words, keeping 118169 word types\n",
      "2019-05-11 10:48:16,344 : INFO : PROGRESS: at sentence #160000, processed 26390910 words, keeping 118665 word types\n",
      "2019-05-11 10:48:16,541 : INFO : PROGRESS: at sentence #170000, processed 27913916 words, keeping 123350 word types\n",
      "2019-05-11 10:48:16,786 : INFO : PROGRESS: at sentence #180000, processed 29535612 words, keeping 126742 word types\n",
      "2019-05-11 10:48:16,994 : INFO : PROGRESS: at sentence #190000, processed 31096459 words, keeping 129841 word types\n",
      "2019-05-11 10:48:17,221 : INFO : PROGRESS: at sentence #200000, processed 32805271 words, keeping 133249 word types\n",
      "2019-05-11 10:48:17,424 : INFO : PROGRESS: at sentence #210000, processed 34434198 words, keeping 136358 word types\n",
      "2019-05-11 10:48:17,639 : INFO : PROGRESS: at sentence #220000, processed 36083482 words, keeping 139412 word types\n",
      "2019-05-11 10:48:17,825 : INFO : PROGRESS: at sentence #230000, processed 37571762 words, keeping 142393 word types\n",
      "2019-05-11 10:48:18,024 : INFO : PROGRESS: at sentence #240000, processed 39138190 words, keeping 145226 word types\n",
      "2019-05-11 10:48:18,226 : INFO : PROGRESS: at sentence #250000, processed 40695049 words, keeping 147960 word types\n",
      "2019-05-11 10:48:18,342 : INFO : collected 150053 word types from a corpus of 41519355 raw words and 255404 sentences\n",
      "2019-05-11 10:48:18,342 : INFO : Loading a fresh vocabulary\n",
      "2019-05-11 10:48:19,077 : INFO : min_count=2 retains 70538 unique words (47% of original 150053, drops 79515)\n",
      "2019-05-11 10:48:19,077 : INFO : min_count=2 leaves 41439840 word corpus (99% of original 41519355, drops 79515)\n",
      "2019-05-11 10:48:19,391 : INFO : deleting the raw counts dictionary of 150053 items\n",
      "2019-05-11 10:48:19,407 : INFO : sample=0.001 downsamples 55 most-common words\n",
      "2019-05-11 10:48:19,407 : INFO : downsampling leaves estimated 30349255 word corpus (73.2% of prior 41439840)\n",
      "2019-05-11 10:48:19,609 : INFO : estimated required memory for 70538 words and 148 dimensions: 118785992 bytes\n",
      "2019-05-11 10:48:19,609 : INFO : resetting layer weights\n",
      "2019-05-11 10:48:20,307 : INFO : training model with 10 workers on 70538 vocabulary and 148 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2019-05-11 10:48:21,318 : INFO : EPOCH 1 - PROGRESS: at 4.95% examples, 1514204 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:48:22,319 : INFO : EPOCH 1 - PROGRESS: at 10.04% examples, 1593157 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:48:23,321 : INFO : EPOCH 1 - PROGRESS: at 14.69% examples, 1615645 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:48:24,336 : INFO : EPOCH 1 - PROGRESS: at 19.33% examples, 1631420 words/s, in_qsize 20, out_qsize 0\n",
      "2019-05-11 10:48:25,335 : INFO : EPOCH 1 - PROGRESS: at 23.31% examples, 1589372 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:48:26,341 : INFO : EPOCH 1 - PROGRESS: at 28.94% examples, 1602045 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:48:27,350 : INFO : EPOCH 1 - PROGRESS: at 34.79% examples, 1612023 words/s, in_qsize 20, out_qsize 1\n",
      "2019-05-11 10:48:28,359 : INFO : EPOCH 1 - PROGRESS: at 40.58% examples, 1612982 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:48:29,352 : INFO : EPOCH 1 - PROGRESS: at 46.18% examples, 1602096 words/s, in_qsize 20, out_qsize 0\n",
      "2019-05-11 10:48:30,386 : INFO : EPOCH 1 - PROGRESS: at 51.04% examples, 1579832 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:48:31,379 : INFO : EPOCH 1 - PROGRESS: at 55.91% examples, 1566714 words/s, in_qsize 19, out_qsize 1\n",
      "2019-05-11 10:48:32,397 : INFO : EPOCH 1 - PROGRESS: at 61.64% examples, 1574678 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:48:33,399 : INFO : EPOCH 1 - PROGRESS: at 67.52% examples, 1582328 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:48:34,401 : INFO : EPOCH 1 - PROGRESS: at 73.10% examples, 1588896 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:48:35,406 : INFO : EPOCH 1 - PROGRESS: at 78.47% examples, 1593331 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:48:36,400 : INFO : EPOCH 1 - PROGRESS: at 83.28% examples, 1584586 words/s, in_qsize 20, out_qsize 0\n",
      "2019-05-11 10:48:37,420 : INFO : EPOCH 1 - PROGRESS: at 88.91% examples, 1587178 words/s, in_qsize 17, out_qsize 2\n",
      "2019-05-11 10:48:38,409 : INFO : EPOCH 1 - PROGRESS: at 94.64% examples, 1591538 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:48:39,328 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-05-11 10:48:39,328 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-05-11 10:48:39,337 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-05-11 10:48:39,338 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-05-11 10:48:39,341 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-05-11 10:48:39,343 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-05-11 10:48:39,347 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-05-11 10:48:39,354 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-11 10:48:39,356 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-11 10:48:39,357 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-11 10:48:39,357 : INFO : EPOCH - 1 : training on 41519355 raw words (30345061 effective words) took 19.0s, 1594540 effective words/s\n",
      "2019-05-11 10:48:40,369 : INFO : EPOCH 2 - PROGRESS: at 5.33% examples, 1630429 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:48:41,378 : INFO : EPOCH 2 - PROGRESS: at 9.87% examples, 1560305 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:48:42,384 : INFO : EPOCH 2 - PROGRESS: at 13.77% examples, 1506684 words/s, in_qsize 18, out_qsize 0\n",
      "2019-05-11 10:48:43,390 : INFO : EPOCH 2 - PROGRESS: at 18.50% examples, 1545446 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:48:44,396 : INFO : EPOCH 2 - PROGRESS: at 23.08% examples, 1569637 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:48:45,403 : INFO : EPOCH 2 - PROGRESS: at 28.07% examples, 1558722 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:48:46,411 : INFO : EPOCH 2 - PROGRESS: at 33.38% examples, 1546471 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:48:47,413 : INFO : EPOCH 2 - PROGRESS: at 38.62% examples, 1541046 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:48:48,418 : INFO : EPOCH 2 - PROGRESS: at 44.22% examples, 1541298 words/s, in_qsize 19, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-11 10:48:49,423 : INFO : EPOCH 2 - PROGRESS: at 50.14% examples, 1554094 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:48:50,428 : INFO : EPOCH 2 - PROGRESS: at 55.70% examples, 1562267 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:48:51,418 : INFO : EPOCH 2 - PROGRESS: at 60.86% examples, 1556007 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:48:52,435 : INFO : EPOCH 2 - PROGRESS: at 66.33% examples, 1556324 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:48:53,430 : INFO : EPOCH 2 - PROGRESS: at 70.95% examples, 1545718 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:48:54,450 : INFO : EPOCH 2 - PROGRESS: at 76.58% examples, 1553741 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:48:55,452 : INFO : EPOCH 2 - PROGRESS: at 81.41% examples, 1550573 words/s, in_qsize 17, out_qsize 2\n",
      "2019-05-11 10:48:56,456 : INFO : EPOCH 2 - PROGRESS: at 86.53% examples, 1549640 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:48:57,469 : INFO : EPOCH 2 - PROGRESS: at 92.27% examples, 1551878 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:48:58,474 : INFO : EPOCH 2 - PROGRESS: at 97.59% examples, 1552445 words/s, in_qsize 20, out_qsize 0\n",
      "2019-05-11 10:48:58,948 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-05-11 10:48:58,964 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-05-11 10:48:58,968 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-05-11 10:48:58,970 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-05-11 10:48:58,974 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-05-11 10:48:58,976 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-05-11 10:48:58,979 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-05-11 10:48:58,981 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-11 10:48:58,984 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-11 10:48:58,989 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-11 10:48:58,990 : INFO : EPOCH - 2 : training on 41519355 raw words (30348656 effective words) took 19.6s, 1546777 effective words/s\n",
      "2019-05-11 10:49:00,002 : INFO : EPOCH 3 - PROGRESS: at 5.24% examples, 1592945 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:49:01,006 : INFO : EPOCH 3 - PROGRESS: at 10.15% examples, 1605543 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:49:02,028 : INFO : EPOCH 3 - PROGRESS: at 14.76% examples, 1611769 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:49:03,024 : INFO : EPOCH 3 - PROGRESS: at 19.33% examples, 1624160 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:49:04,059 : INFO : EPOCH 3 - PROGRESS: at 23.57% examples, 1595640 words/s, in_qsize 17, out_qsize 2\n",
      "2019-05-11 10:49:05,065 : INFO : EPOCH 3 - PROGRESS: at 28.24% examples, 1558155 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:49:06,070 : INFO : EPOCH 3 - PROGRESS: at 33.38% examples, 1539352 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:49:07,073 : INFO : EPOCH 3 - PROGRESS: at 38.68% examples, 1536751 words/s, in_qsize 17, out_qsize 2\n",
      "2019-05-11 10:49:08,075 : INFO : EPOCH 3 - PROGRESS: at 44.51% examples, 1543671 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:49:09,089 : INFO : EPOCH 3 - PROGRESS: at 50.13% examples, 1548267 words/s, in_qsize 17, out_qsize 2\n",
      "2019-05-11 10:49:10,094 : INFO : EPOCH 3 - PROGRESS: at 54.57% examples, 1529412 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:49:11,096 : INFO : EPOCH 3 - PROGRESS: at 59.47% examples, 1517963 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:49:12,106 : INFO : EPOCH 3 - PROGRESS: at 65.02% examples, 1519623 words/s, in_qsize 17, out_qsize 2\n",
      "2019-05-11 10:49:13,107 : INFO : EPOCH 3 - PROGRESS: at 70.32% examples, 1527419 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:49:14,113 : INFO : EPOCH 3 - PROGRESS: at 75.86% examples, 1534444 words/s, in_qsize 17, out_qsize 2\n",
      "2019-05-11 10:49:15,121 : INFO : EPOCH 3 - PROGRESS: at 81.09% examples, 1540233 words/s, in_qsize 16, out_qsize 3\n",
      "2019-05-11 10:49:16,117 : INFO : EPOCH 3 - PROGRESS: at 85.54% examples, 1529422 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:49:17,135 : INFO : EPOCH 3 - PROGRESS: at 91.39% examples, 1534968 words/s, in_qsize 17, out_qsize 2\n",
      "2019-05-11 10:49:18,145 : INFO : EPOCH 3 - PROGRESS: at 97.02% examples, 1540240 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:49:18,615 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-05-11 10:49:18,630 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-05-11 10:49:18,630 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-05-11 10:49:18,630 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-05-11 10:49:18,640 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-05-11 10:49:18,643 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-05-11 10:49:18,645 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-05-11 10:49:18,648 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-11 10:49:18,653 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-11 10:49:18,655 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-11 10:49:18,655 : INFO : EPOCH - 3 : training on 41519355 raw words (30349459 effective words) took 19.7s, 1543719 effective words/s\n",
      "2019-05-11 10:49:19,659 : INFO : EPOCH 4 - PROGRESS: at 5.29% examples, 1624794 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:49:20,663 : INFO : EPOCH 4 - PROGRESS: at 10.13% examples, 1607208 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:49:21,678 : INFO : EPOCH 4 - PROGRESS: at 13.79% examples, 1509139 words/s, in_qsize 20, out_qsize 0\n",
      "2019-05-11 10:49:22,675 : INFO : EPOCH 4 - PROGRESS: at 18.28% examples, 1527233 words/s, in_qsize 17, out_qsize 2\n",
      "2019-05-11 10:49:23,682 : INFO : EPOCH 4 - PROGRESS: at 22.60% examples, 1532118 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:49:24,691 : INFO : EPOCH 4 - PROGRESS: at 27.63% examples, 1537761 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:49:25,690 : INFO : EPOCH 4 - PROGRESS: at 33.33% examples, 1543671 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:49:26,708 : INFO : EPOCH 4 - PROGRESS: at 38.53% examples, 1537605 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:49:27,711 : INFO : EPOCH 4 - PROGRESS: at 43.57% examples, 1521522 words/s, in_qsize 20, out_qsize 0\n",
      "2019-05-11 10:49:28,706 : INFO : EPOCH 4 - PROGRESS: at 49.14% examples, 1525673 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:49:29,721 : INFO : EPOCH 4 - PROGRESS: at 54.35% examples, 1527967 words/s, in_qsize 17, out_qsize 2\n",
      "2019-05-11 10:49:30,712 : INFO : EPOCH 4 - PROGRESS: at 59.86% examples, 1531747 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:49:31,721 : INFO : EPOCH 4 - PROGRESS: at 65.39% examples, 1532994 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:49:32,743 : INFO : EPOCH 4 - PROGRESS: at 69.81% examples, 1518165 words/s, in_qsize 17, out_qsize 2\n",
      "2019-05-11 10:49:33,757 : INFO : EPOCH 4 - PROGRESS: at 75.11% examples, 1520637 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:49:34,755 : INFO : EPOCH 4 - PROGRESS: at 80.07% examples, 1522922 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:49:35,756 : INFO : EPOCH 4 - PROGRESS: at 85.20% examples, 1525405 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:49:36,773 : INFO : EPOCH 4 - PROGRESS: at 90.89% examples, 1528801 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:49:37,771 : INFO : EPOCH 4 - PROGRESS: at 96.09% examples, 1528737 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:49:38,585 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-05-11 10:49:38,585 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-05-11 10:49:38,605 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-05-11 10:49:38,605 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-05-11 10:49:38,606 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-05-11 10:49:38,607 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-05-11 10:49:38,611 : INFO : worker thread finished; awaiting finish of 3 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-11 10:49:38,616 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-11 10:49:38,623 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-11 10:49:38,625 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-11 10:49:38,626 : INFO : EPOCH - 4 : training on 41519355 raw words (30350718 effective words) took 20.0s, 1519950 effective words/s\n",
      "2019-05-11 10:49:39,634 : INFO : EPOCH 5 - PROGRESS: at 5.00% examples, 1520745 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:49:40,637 : INFO : EPOCH 5 - PROGRESS: at 9.85% examples, 1551546 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:49:41,638 : INFO : EPOCH 5 - PROGRESS: at 14.22% examples, 1556391 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:49:42,639 : INFO : EPOCH 5 - PROGRESS: at 18.64% examples, 1561414 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:49:43,653 : INFO : EPOCH 5 - PROGRESS: at 22.68% examples, 1534235 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:49:44,655 : INFO : EPOCH 5 - PROGRESS: at 27.22% examples, 1520979 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:49:45,657 : INFO : EPOCH 5 - PROGRESS: at 32.88% examples, 1526684 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:49:46,671 : INFO : EPOCH 5 - PROGRESS: at 38.26% examples, 1530140 words/s, in_qsize 20, out_qsize 0\n",
      "2019-05-11 10:49:47,669 : INFO : EPOCH 5 - PROGRESS: at 44.03% examples, 1535644 words/s, in_qsize 19, out_qsize 1\n",
      "2019-05-11 10:49:48,672 : INFO : EPOCH 5 - PROGRESS: at 49.57% examples, 1538931 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:49:49,692 : INFO : EPOCH 5 - PROGRESS: at 53.65% examples, 1510337 words/s, in_qsize 17, out_qsize 2\n",
      "2019-05-11 10:49:50,700 : INFO : EPOCH 5 - PROGRESS: at 58.73% examples, 1504520 words/s, in_qsize 17, out_qsize 2\n",
      "2019-05-11 10:49:51,700 : INFO : EPOCH 5 - PROGRESS: at 63.92% examples, 1501773 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:49:52,707 : INFO : EPOCH 5 - PROGRESS: at 68.55% examples, 1491820 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:49:53,705 : INFO : EPOCH 5 - PROGRESS: at 73.45% examples, 1489327 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:49:54,706 : INFO : EPOCH 5 - PROGRESS: at 78.23% examples, 1490671 words/s, in_qsize 20, out_qsize 0\n",
      "2019-05-11 10:49:55,710 : INFO : EPOCH 5 - PROGRESS: at 82.23% examples, 1473999 words/s, in_qsize 17, out_qsize 2\n",
      "2019-05-11 10:49:56,722 : INFO : EPOCH 5 - PROGRESS: at 87.09% examples, 1471905 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:49:57,718 : INFO : EPOCH 5 - PROGRESS: at 92.23% examples, 1470114 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:49:58,734 : INFO : EPOCH 5 - PROGRESS: at 97.32% examples, 1471017 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:49:59,236 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-05-11 10:49:59,259 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-05-11 10:49:59,263 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-05-11 10:49:59,265 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-05-11 10:49:59,266 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-05-11 10:49:59,267 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-05-11 10:49:59,275 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-05-11 10:49:59,284 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-11 10:49:59,286 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-11 10:49:59,286 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-11 10:49:59,287 : INFO : EPOCH - 5 : training on 41519355 raw words (30345673 effective words) took 20.7s, 1469161 effective words/s\n",
      "2019-05-11 10:49:59,287 : INFO : training on a 207596775 raw words (151739567 effective words) took 99.0s, 1533244 effective words/s\n",
      "2019-05-11 10:49:59,308 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2019-05-11 10:49:59,309 : INFO : training model with 10 workers on 70538 vocabulary and 148 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2019-05-11 10:50:00,325 : INFO : EPOCH 1 - PROGRESS: at 4.34% examples, 1323135 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:50:01,332 : INFO : EPOCH 1 - PROGRESS: at 8.65% examples, 1325645 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:50:02,335 : INFO : EPOCH 1 - PROGRESS: at 12.66% examples, 1388286 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:50:03,342 : INFO : EPOCH 1 - PROGRESS: at 17.14% examples, 1423978 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:50:04,351 : INFO : EPOCH 1 - PROGRESS: at 21.54% examples, 1449053 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:50:05,370 : INFO : EPOCH 1 - PROGRESS: at 25.96% examples, 1460863 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:50:06,376 : INFO : EPOCH 1 - PROGRESS: at 31.02% examples, 1449977 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:50:07,368 : INFO : EPOCH 1 - PROGRESS: at 36.29% examples, 1455449 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:50:08,381 : INFO : EPOCH 1 - PROGRESS: at 41.92% examples, 1467193 words/s, in_qsize 17, out_qsize 2\n",
      "2019-05-11 10:50:09,389 : INFO : EPOCH 1 - PROGRESS: at 47.37% examples, 1474277 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:50:10,383 : INFO : EPOCH 1 - PROGRESS: at 52.76% examples, 1482522 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:50:11,399 : INFO : EPOCH 1 - PROGRESS: at 58.12% examples, 1488136 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:50:12,409 : INFO : EPOCH 1 - PROGRESS: at 62.66% examples, 1473938 words/s, in_qsize 17, out_qsize 2\n",
      "2019-05-11 10:50:13,414 : INFO : EPOCH 1 - PROGRESS: at 68.13% examples, 1479620 words/s, in_qsize 17, out_qsize 2\n",
      "2019-05-11 10:50:14,414 : INFO : EPOCH 1 - PROGRESS: at 73.40% examples, 1485571 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:50:15,431 : INFO : EPOCH 1 - PROGRESS: at 78.14% examples, 1485144 words/s, in_qsize 17, out_qsize 2\n",
      "2019-05-11 10:50:16,441 : INFO : EPOCH 1 - PROGRESS: at 83.09% examples, 1485026 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:50:17,467 : INFO : EPOCH 1 - PROGRESS: at 87.82% examples, 1477719 words/s, in_qsize 17, out_qsize 3\n",
      "2019-05-11 10:50:18,483 : INFO : EPOCH 1 - PROGRESS: at 92.97% examples, 1477144 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:50:19,495 : INFO : EPOCH 1 - PROGRESS: at 98.30% examples, 1480060 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:50:19,762 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-05-11 10:50:19,762 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-05-11 10:50:19,779 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-05-11 10:50:19,784 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-05-11 10:50:19,792 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-05-11 10:50:19,793 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-05-11 10:50:19,799 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-05-11 10:50:19,802 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-11 10:50:19,803 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-11 10:50:19,804 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-11 10:50:19,805 : INFO : EPOCH - 1 : training on 41519355 raw words (30350678 effective words) took 20.5s, 1481313 effective words/s\n",
      "2019-05-11 10:50:20,810 : INFO : EPOCH 2 - PROGRESS: at 4.75% examples, 1461146 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:50:21,815 : INFO : EPOCH 2 - PROGRESS: at 8.65% examples, 1334039 words/s, in_qsize 16, out_qsize 3\n",
      "2019-05-11 10:50:22,817 : INFO : EPOCH 2 - PROGRESS: at 12.47% examples, 1370612 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:50:23,826 : INFO : EPOCH 2 - PROGRESS: at 16.26% examples, 1345236 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:50:24,832 : INFO : EPOCH 2 - PROGRESS: at 20.37% examples, 1384054 words/s, in_qsize 17, out_qsize 2\n",
      "2019-05-11 10:50:25,838 : INFO : EPOCH 2 - PROGRESS: at 24.67% examples, 1409767 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:50:26,844 : INFO : EPOCH 2 - PROGRESS: at 30.28% examples, 1428544 words/s, in_qsize 18, out_qsize 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-11 10:50:27,840 : INFO : EPOCH 2 - PROGRESS: at 35.72% examples, 1441900 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:50:28,848 : INFO : EPOCH 2 - PROGRESS: at 40.46% examples, 1430252 words/s, in_qsize 20, out_qsize 0\n",
      "2019-05-11 10:50:29,846 : INFO : EPOCH 2 - PROGRESS: at 45.95% examples, 1436437 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:50:30,853 : INFO : EPOCH 2 - PROGRESS: at 50.85% examples, 1434342 words/s, in_qsize 17, out_qsize 2\n",
      "2019-05-11 10:50:31,848 : INFO : EPOCH 2 - PROGRESS: at 55.63% examples, 1432134 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:50:32,860 : INFO : EPOCH 2 - PROGRESS: at 60.72% examples, 1435308 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:50:33,860 : INFO : EPOCH 2 - PROGRESS: at 65.98% examples, 1440090 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:50:34,865 : INFO : EPOCH 2 - PROGRESS: at 70.35% examples, 1431263 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:50:35,893 : INFO : EPOCH 2 - PROGRESS: at 75.42% examples, 1433692 words/s, in_qsize 16, out_qsize 3\n",
      "2019-05-11 10:50:36,880 : INFO : EPOCH 2 - PROGRESS: at 80.26% examples, 1439011 words/s, in_qsize 16, out_qsize 3\n",
      "2019-05-11 10:50:37,894 : INFO : EPOCH 2 - PROGRESS: at 85.11% examples, 1441678 words/s, in_qsize 17, out_qsize 2\n",
      "2019-05-11 10:50:38,914 : INFO : EPOCH 2 - PROGRESS: at 90.52% examples, 1444982 words/s, in_qsize 16, out_qsize 3\n",
      "2019-05-11 10:50:39,921 : INFO : EPOCH 2 - PROGRESS: at 94.97% examples, 1437133 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:50:40,914 : INFO : EPOCH 2 - PROGRESS: at 99.26% examples, 1427780 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:50:41,030 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-05-11 10:50:41,035 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-05-11 10:50:41,036 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-05-11 10:50:41,039 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-05-11 10:50:41,041 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-05-11 10:50:41,042 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-05-11 10:50:41,050 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-05-11 10:50:41,060 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-11 10:50:41,062 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-11 10:50:41,064 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-11 10:50:41,065 : INFO : EPOCH - 2 : training on 41519355 raw words (30353267 effective words) took 21.3s, 1428198 effective words/s\n",
      "2019-05-11 10:50:42,087 : INFO : EPOCH 3 - PROGRESS: at 4.38% examples, 1325335 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:50:43,091 : INFO : EPOCH 3 - PROGRESS: at 8.94% examples, 1377047 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:50:44,084 : INFO : EPOCH 3 - PROGRESS: at 12.82% examples, 1398973 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:50:45,093 : INFO : EPOCH 3 - PROGRESS: at 17.25% examples, 1430733 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:50:46,090 : INFO : EPOCH 3 - PROGRESS: at 20.24% examples, 1372789 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:50:47,099 : INFO : EPOCH 3 - PROGRESS: at 23.97% examples, 1365289 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:50:48,103 : INFO : EPOCH 3 - PROGRESS: at 29.30% examples, 1385381 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:50:49,110 : INFO : EPOCH 3 - PROGRESS: at 34.65% examples, 1400782 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:50:50,123 : INFO : EPOCH 3 - PROGRESS: at 40.00% examples, 1413517 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:50:51,130 : INFO : EPOCH 3 - PROGRESS: at 45.30% examples, 1414607 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:50:52,141 : INFO : EPOCH 3 - PROGRESS: at 50.05% examples, 1408432 words/s, in_qsize 18, out_qsize 2\n",
      "2019-05-11 10:50:53,159 : INFO : EPOCH 3 - PROGRESS: at 55.13% examples, 1416826 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:50:54,174 : INFO : EPOCH 3 - PROGRESS: at 60.48% examples, 1423845 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:50:55,191 : INFO : EPOCH 3 - PROGRESS: at 65.39% examples, 1419678 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:50:56,208 : INFO : EPOCH 3 - PROGRESS: at 69.77% examples, 1411591 words/s, in_qsize 19, out_qsize 1\n",
      "2019-05-11 10:50:57,215 : INFO : EPOCH 3 - PROGRESS: at 73.84% examples, 1397711 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:50:58,224 : INFO : EPOCH 3 - PROGRESS: at 77.95% examples, 1391222 words/s, in_qsize 18, out_qsize 2\n",
      "2019-05-11 10:50:59,239 : INFO : EPOCH 3 - PROGRESS: at 82.42% examples, 1388961 words/s, in_qsize 20, out_qsize 1\n",
      "2019-05-11 10:51:00,242 : INFO : EPOCH 3 - PROGRESS: at 87.36% examples, 1393254 words/s, in_qsize 17, out_qsize 2\n",
      "2019-05-11 10:51:01,239 : INFO : EPOCH 3 - PROGRESS: at 92.64% examples, 1398415 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:51:02,249 : INFO : EPOCH 3 - PROGRESS: at 97.48% examples, 1398875 words/s, in_qsize 17, out_qsize 2\n",
      "2019-05-11 10:51:02,756 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-05-11 10:51:02,771 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-05-11 10:51:02,787 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-05-11 10:51:02,791 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-05-11 10:51:02,795 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-05-11 10:51:02,797 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-05-11 10:51:02,799 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-05-11 10:51:02,800 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-11 10:51:02,803 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-11 10:51:02,805 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-11 10:51:02,807 : INFO : EPOCH - 3 : training on 41519355 raw words (30350002 effective words) took 21.7s, 1396332 effective words/s\n",
      "2019-05-11 10:51:03,818 : INFO : EPOCH 4 - PROGRESS: at 4.65% examples, 1433521 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:51:04,823 : INFO : EPOCH 4 - PROGRESS: at 9.36% examples, 1455529 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:51:05,833 : INFO : EPOCH 4 - PROGRESS: at 13.34% examples, 1454725 words/s, in_qsize 20, out_qsize 0\n",
      "2019-05-11 10:51:06,838 : INFO : EPOCH 4 - PROGRESS: at 17.63% examples, 1468380 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:51:07,844 : INFO : EPOCH 4 - PROGRESS: at 21.59% examples, 1454424 words/s, in_qsize 17, out_qsize 2\n",
      "2019-05-11 10:51:08,838 : INFO : EPOCH 4 - PROGRESS: at 25.29% examples, 1435383 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:51:09,853 : INFO : EPOCH 4 - PROGRESS: at 30.52% examples, 1434878 words/s, in_qsize 15, out_qsize 4\n",
      "2019-05-11 10:51:10,854 : INFO : EPOCH 4 - PROGRESS: at 35.88% examples, 1445599 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:51:11,855 : INFO : EPOCH 4 - PROGRESS: at 41.17% examples, 1447934 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:51:12,873 : INFO : EPOCH 4 - PROGRESS: at 46.36% examples, 1445743 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:51:13,870 : INFO : EPOCH 4 - PROGRESS: at 50.23% examples, 1415001 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:51:14,871 : INFO : EPOCH 4 - PROGRESS: at 54.30% examples, 1400031 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:51:15,888 : INFO : EPOCH 4 - PROGRESS: at 59.47% examples, 1404712 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:51:16,886 : INFO : EPOCH 4 - PROGRESS: at 64.84% examples, 1410572 words/s, in_qsize 20, out_qsize 0\n",
      "2019-05-11 10:51:17,904 : INFO : EPOCH 4 - PROGRESS: at 69.78% examples, 1416443 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:51:18,901 : INFO : EPOCH 4 - PROGRESS: at 74.56% examples, 1415807 words/s, in_qsize 16, out_qsize 3\n",
      "2019-05-11 10:51:19,903 : INFO : EPOCH 4 - PROGRESS: at 78.80% examples, 1411072 words/s, in_qsize 20, out_qsize 2\n",
      "2019-05-11 10:51:20,904 : INFO : EPOCH 4 - PROGRESS: at 83.71% examples, 1415304 words/s, in_qsize 19, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-11 10:51:21,917 : INFO : EPOCH 4 - PROGRESS: at 88.82% examples, 1418733 words/s, in_qsize 17, out_qsize 2\n",
      "2019-05-11 10:51:22,917 : INFO : EPOCH 4 - PROGRESS: at 93.92% examples, 1421957 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:51:23,927 : INFO : EPOCH 4 - PROGRESS: at 98.66% examples, 1419057 words/s, in_qsize 16, out_qsize 3\n",
      "2019-05-11 10:51:24,175 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-05-11 10:51:24,180 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-05-11 10:51:24,182 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-05-11 10:51:24,185 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-05-11 10:51:24,191 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-05-11 10:51:24,194 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-05-11 10:51:24,196 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-05-11 10:51:24,197 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-11 10:51:24,204 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-11 10:51:24,206 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-11 10:51:24,207 : INFO : EPOCH - 4 : training on 41519355 raw words (30344086 effective words) took 21.4s, 1418693 effective words/s\n",
      "2019-05-11 10:51:25,223 : INFO : EPOCH 5 - PROGRESS: at 3.94% examples, 1210654 words/s, in_qsize 20, out_qsize 0\n",
      "2019-05-11 10:51:26,234 : INFO : EPOCH 5 - PROGRESS: at 8.79% examples, 1348393 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:51:27,252 : INFO : EPOCH 5 - PROGRESS: at 12.63% examples, 1372877 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:51:28,253 : INFO : EPOCH 5 - PROGRESS: at 17.02% examples, 1403281 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:51:29,261 : INFO : EPOCH 5 - PROGRESS: at 20.80% examples, 1414150 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:51:30,259 : INFO : EPOCH 5 - PROGRESS: at 24.55% examples, 1398702 words/s, in_qsize 17, out_qsize 2\n",
      "2019-05-11 10:51:31,274 : INFO : EPOCH 5 - PROGRESS: at 29.50% examples, 1390124 words/s, in_qsize 20, out_qsize 0\n",
      "2019-05-11 10:51:32,267 : INFO : EPOCH 5 - PROGRESS: at 34.79% examples, 1404614 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:51:33,280 : INFO : EPOCH 5 - PROGRESS: at 39.94% examples, 1409969 words/s, in_qsize 20, out_qsize 0\n",
      "2019-05-11 10:51:34,288 : INFO : EPOCH 5 - PROGRESS: at 44.91% examples, 1403107 words/s, in_qsize 17, out_qsize 2\n",
      "2019-05-11 10:51:35,288 : INFO : EPOCH 5 - PROGRESS: at 49.68% examples, 1400592 words/s, in_qsize 18, out_qsize 2\n",
      "2019-05-11 10:51:36,283 : INFO : EPOCH 5 - PROGRESS: at 54.10% examples, 1394225 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:51:37,296 : INFO : EPOCH 5 - PROGRESS: at 59.25% examples, 1399882 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:51:38,285 : INFO : EPOCH 5 - PROGRESS: at 64.51% examples, 1404147 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:51:39,298 : INFO : EPOCH 5 - PROGRESS: at 69.35% examples, 1407831 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:51:40,299 : INFO : EPOCH 5 - PROGRESS: at 74.34% examples, 1412207 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:51:41,305 : INFO : EPOCH 5 - PROGRESS: at 78.29% examples, 1402904 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:51:42,309 : INFO : EPOCH 5 - PROGRESS: at 82.59% examples, 1397220 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:51:43,322 : INFO : EPOCH 5 - PROGRESS: at 87.24% examples, 1396229 words/s, in_qsize 20, out_qsize 0\n",
      "2019-05-11 10:51:44,323 : INFO : EPOCH 5 - PROGRESS: at 92.26% examples, 1397135 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:51:45,317 : INFO : EPOCH 5 - PROGRESS: at 97.13% examples, 1398840 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:51:45,859 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-05-11 10:51:45,875 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-05-11 10:51:45,894 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-05-11 10:51:45,895 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-05-11 10:51:45,902 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-05-11 10:51:45,906 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-05-11 10:51:45,909 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-05-11 10:51:45,911 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-11 10:51:45,912 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-11 10:51:45,918 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-11 10:51:45,920 : INFO : EPOCH - 5 : training on 41519355 raw words (30351744 effective words) took 21.7s, 1398567 effective words/s\n",
      "2019-05-11 10:51:46,943 : INFO : EPOCH 6 - PROGRESS: at 4.44% examples, 1350809 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:51:47,933 : INFO : EPOCH 6 - PROGRESS: at 8.53% examples, 1307226 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:51:48,951 : INFO : EPOCH 6 - PROGRESS: at 12.47% examples, 1359955 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:51:49,948 : INFO : EPOCH 6 - PROGRESS: at 16.76% examples, 1385035 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:51:50,962 : INFO : EPOCH 6 - PROGRESS: at 20.60% examples, 1400586 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:51:51,965 : INFO : EPOCH 6 - PROGRESS: at 24.47% examples, 1395782 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:51:52,978 : INFO : EPOCH 6 - PROGRESS: at 29.38% examples, 1385773 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:51:53,979 : INFO : EPOCH 6 - PROGRESS: at 34.47% examples, 1391972 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:51:54,981 : INFO : EPOCH 6 - PROGRESS: at 39.60% examples, 1400044 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:51:55,994 : INFO : EPOCH 6 - PROGRESS: at 44.96% examples, 1404471 words/s, in_qsize 20, out_qsize 0\n",
      "2019-05-11 10:51:56,995 : INFO : EPOCH 6 - PROGRESS: at 49.86% examples, 1404228 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:51:58,012 : INFO : EPOCH 6 - PROGRESS: at 54.61% examples, 1405462 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:51:59,011 : INFO : EPOCH 6 - PROGRESS: at 59.23% examples, 1398125 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:52:00,012 : INFO : EPOCH 6 - PROGRESS: at 64.42% examples, 1401156 words/s, in_qsize 17, out_qsize 2\n",
      "2019-05-11 10:52:01,019 : INFO : EPOCH 6 - PROGRESS: at 69.21% examples, 1403790 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:52:02,028 : INFO : EPOCH 6 - PROGRESS: at 73.27% examples, 1391929 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:52:03,034 : INFO : EPOCH 6 - PROGRESS: at 77.50% examples, 1386877 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:52:04,039 : INFO : EPOCH 6 - PROGRESS: at 81.92% examples, 1384708 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:52:05,054 : INFO : EPOCH 6 - PROGRESS: at 86.41% examples, 1382794 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:52:06,061 : INFO : EPOCH 6 - PROGRESS: at 91.53% examples, 1385118 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:52:07,058 : INFO : EPOCH 6 - PROGRESS: at 96.35% examples, 1386264 words/s, in_qsize 16, out_qsize 3\n",
      "2019-05-11 10:52:07,753 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-05-11 10:52:07,761 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-05-11 10:52:07,765 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-05-11 10:52:07,767 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-05-11 10:52:07,773 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-05-11 10:52:07,786 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-05-11 10:52:07,790 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-05-11 10:52:07,791 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-11 10:52:07,792 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-11 10:52:07,792 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-11 10:52:07,793 : INFO : EPOCH - 6 : training on 41519355 raw words (30349473 effective words) took 21.9s, 1388008 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-11 10:52:08,801 : INFO : EPOCH 7 - PROGRESS: at 4.30% examples, 1317806 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:52:09,808 : INFO : EPOCH 7 - PROGRESS: at 8.01% examples, 1237210 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:52:10,809 : INFO : EPOCH 7 - PROGRESS: at 11.84% examples, 1284659 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:52:11,825 : INFO : EPOCH 7 - PROGRESS: at 15.55% examples, 1279241 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:52:12,832 : INFO : EPOCH 7 - PROGRESS: at 19.17% examples, 1287975 words/s, in_qsize 17, out_qsize 2\n",
      "2019-05-11 10:52:13,833 : INFO : EPOCH 7 - PROGRESS: at 22.98% examples, 1298022 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:52:14,842 : INFO : EPOCH 7 - PROGRESS: at 27.50% examples, 1314797 words/s, in_qsize 17, out_qsize 2\n",
      "2019-05-11 10:52:15,844 : INFO : EPOCH 7 - PROGRESS: at 32.29% examples, 1313010 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:52:16,874 : INFO : EPOCH 7 - PROGRESS: at 37.23% examples, 1324081 words/s, in_qsize 19, out_qsize 2\n",
      "2019-05-11 10:52:17,891 : INFO : EPOCH 7 - PROGRESS: at 42.52% examples, 1335868 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:52:18,906 : INFO : EPOCH 7 - PROGRESS: at 47.60% examples, 1341932 words/s, in_qsize 19, out_qsize 2\n",
      "2019-05-11 10:52:19,914 : INFO : EPOCH 7 - PROGRESS: at 52.42% examples, 1345580 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:52:20,922 : INFO : EPOCH 7 - PROGRESS: at 56.82% examples, 1341297 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:52:21,922 : INFO : EPOCH 7 - PROGRESS: at 61.68% examples, 1345876 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:52:22,924 : INFO : EPOCH 7 - PROGRESS: at 66.72% examples, 1351777 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:52:23,939 : INFO : EPOCH 7 - PROGRESS: at 71.38% examples, 1355174 words/s, in_qsize 20, out_qsize 0\n",
      "2019-05-11 10:52:24,940 : INFO : EPOCH 7 - PROGRESS: at 75.97% examples, 1354521 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:52:25,957 : INFO : EPOCH 7 - PROGRESS: at 80.54% examples, 1358438 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:52:26,956 : INFO : EPOCH 7 - PROGRESS: at 84.63% examples, 1353239 words/s, in_qsize 16, out_qsize 1\n",
      "2019-05-11 10:52:27,955 : INFO : EPOCH 7 - PROGRESS: at 89.58% examples, 1355249 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:52:28,972 : INFO : EPOCH 7 - PROGRESS: at 94.54% examples, 1358767 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:52:29,972 : INFO : EPOCH 7 - PROGRESS: at 99.35% examples, 1359974 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:52:30,081 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-05-11 10:52:30,081 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-05-11 10:52:30,081 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-05-11 10:52:30,081 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-05-11 10:52:30,096 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-05-11 10:52:30,097 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-05-11 10:52:30,108 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-05-11 10:52:30,110 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-11 10:52:30,112 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-11 10:52:30,116 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-11 10:52:30,118 : INFO : EPOCH - 7 : training on 41519355 raw words (30348447 effective words) took 22.3s, 1359802 effective words/s\n",
      "2019-05-11 10:52:31,135 : INFO : EPOCH 8 - PROGRESS: at 4.38% examples, 1335249 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:52:32,147 : INFO : EPOCH 8 - PROGRESS: at 8.14% examples, 1246931 words/s, in_qsize 17, out_qsize 2\n",
      "2019-05-11 10:52:33,154 : INFO : EPOCH 8 - PROGRESS: at 11.95% examples, 1287306 words/s, in_qsize 20, out_qsize 0\n",
      "2019-05-11 10:52:34,153 : INFO : EPOCH 8 - PROGRESS: at 16.09% examples, 1323166 words/s, in_qsize 17, out_qsize 2\n",
      "2019-05-11 10:52:35,173 : INFO : EPOCH 8 - PROGRESS: at 19.71% examples, 1327421 words/s, in_qsize 16, out_qsize 3\n",
      "2019-05-11 10:52:36,163 : INFO : EPOCH 8 - PROGRESS: at 23.53% examples, 1332690 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:52:37,169 : INFO : EPOCH 8 - PROGRESS: at 28.27% examples, 1341459 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:52:38,191 : INFO : EPOCH 8 - PROGRESS: at 32.80% examples, 1329629 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:52:39,186 : INFO : EPOCH 8 - PROGRESS: at 37.50% examples, 1333213 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:52:40,195 : INFO : EPOCH 8 - PROGRESS: at 42.23% examples, 1329129 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:52:41,211 : INFO : EPOCH 8 - PROGRESS: at 47.03% examples, 1330330 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:52:42,217 : INFO : EPOCH 8 - PROGRESS: at 51.90% examples, 1334622 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:52:43,229 : INFO : EPOCH 8 - PROGRESS: at 56.41% examples, 1334265 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:52:44,236 : INFO : EPOCH 8 - PROGRESS: at 60.88% examples, 1330508 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:52:45,235 : INFO : EPOCH 8 - PROGRESS: at 65.79% examples, 1335228 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:52:46,237 : INFO : EPOCH 8 - PROGRESS: at 70.43% examples, 1338835 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:52:47,251 : INFO : EPOCH 8 - PROGRESS: at 75.00% examples, 1337580 words/s, in_qsize 19, out_qsize 1\n",
      "2019-05-11 10:52:48,256 : INFO : EPOCH 8 - PROGRESS: at 79.47% examples, 1341621 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:52:49,267 : INFO : EPOCH 8 - PROGRESS: at 83.58% examples, 1335956 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:52:50,269 : INFO : EPOCH 8 - PROGRESS: at 88.35% examples, 1338794 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:52:51,266 : INFO : EPOCH 8 - PROGRESS: at 93.14% examples, 1341339 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:52:52,292 : INFO : EPOCH 8 - PROGRESS: at 97.69% examples, 1339658 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:52:52,706 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-05-11 10:52:52,706 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-05-11 10:52:52,739 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-05-11 10:52:52,743 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-05-11 10:52:52,747 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-05-11 10:52:52,749 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-05-11 10:52:52,751 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-05-11 10:52:52,761 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-11 10:52:52,768 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-11 10:52:52,770 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-11 10:52:52,771 : INFO : EPOCH - 8 : training on 41519355 raw words (30351207 effective words) took 22.6s, 1340202 effective words/s\n",
      "2019-05-11 10:52:53,767 : INFO : EPOCH 9 - PROGRESS: at 4.40% examples, 1354614 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:52:54,774 : INFO : EPOCH 9 - PROGRESS: at 8.06% examples, 1248328 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:52:55,773 : INFO : EPOCH 9 - PROGRESS: at 11.77% examples, 1282627 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:52:56,784 : INFO : EPOCH 9 - PROGRESS: at 15.83% examples, 1306197 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:52:57,791 : INFO : EPOCH 9 - PROGRESS: at 19.26% examples, 1299094 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:52:58,805 : INFO : EPOCH 9 - PROGRESS: at 23.07% examples, 1308397 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:52:59,807 : INFO : EPOCH 9 - PROGRESS: at 27.29% examples, 1307278 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:53:00,831 : INFO : EPOCH 9 - PROGRESS: at 31.76% examples, 1293984 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:53:01,831 : INFO : EPOCH 9 - PROGRESS: at 36.68% examples, 1308254 words/s, in_qsize 17, out_qsize 2\n",
      "2019-05-11 10:53:02,839 : INFO : EPOCH 9 - PROGRESS: at 41.86% examples, 1319580 words/s, in_qsize 19, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-11 10:53:03,832 : INFO : EPOCH 9 - PROGRESS: at 46.52% examples, 1318729 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:53:04,846 : INFO : EPOCH 9 - PROGRESS: at 51.42% examples, 1326229 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:53:05,848 : INFO : EPOCH 9 - PROGRESS: at 55.31% examples, 1313514 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:53:06,848 : INFO : EPOCH 9 - PROGRESS: at 59.62% examples, 1308262 words/s, in_qsize 19, out_qsize 2\n",
      "2019-05-11 10:53:07,859 : INFO : EPOCH 9 - PROGRESS: at 64.34% examples, 1308339 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:53:08,863 : INFO : EPOCH 9 - PROGRESS: at 68.92% examples, 1311935 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:53:09,870 : INFO : EPOCH 9 - PROGRESS: at 73.56% examples, 1315259 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:53:10,864 : INFO : EPOCH 9 - PROGRESS: at 78.09% examples, 1321556 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:53:11,880 : INFO : EPOCH 9 - PROGRESS: at 82.28% examples, 1318473 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:53:12,881 : INFO : EPOCH 9 - PROGRESS: at 87.09% examples, 1323938 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:53:13,895 : INFO : EPOCH 9 - PROGRESS: at 92.16% examples, 1328956 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:53:14,903 : INFO : EPOCH 9 - PROGRESS: at 96.05% examples, 1320616 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:53:15,749 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-05-11 10:53:15,749 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-05-11 10:53:15,749 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-05-11 10:53:15,768 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-05-11 10:53:15,770 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-05-11 10:53:15,776 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-05-11 10:53:15,778 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-05-11 10:53:15,779 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-11 10:53:15,785 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-11 10:53:15,789 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-11 10:53:15,791 : INFO : EPOCH - 9 : training on 41519355 raw words (30352028 effective words) took 23.0s, 1318943 effective words/s\n",
      "2019-05-11 10:53:16,792 : INFO : EPOCH 10 - PROGRESS: at 4.19% examples, 1292027 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:53:17,790 : INFO : EPOCH 10 - PROGRESS: at 8.23% examples, 1274632 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:53:18,797 : INFO : EPOCH 10 - PROGRESS: at 12.18% examples, 1329677 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:53:19,807 : INFO : EPOCH 10 - PROGRESS: at 16.14% examples, 1331647 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:53:20,824 : INFO : EPOCH 10 - PROGRESS: at 19.86% examples, 1344043 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:53:21,813 : INFO : EPOCH 10 - PROGRESS: at 23.89% examples, 1362414 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:53:22,826 : INFO : EPOCH 10 - PROGRESS: at 28.44% examples, 1353233 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:53:23,830 : INFO : EPOCH 10 - PROGRESS: at 33.56% examples, 1362749 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:53:24,827 : INFO : EPOCH 10 - PROGRESS: at 38.54% examples, 1370684 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:53:25,826 : INFO : EPOCH 10 - PROGRESS: at 43.54% examples, 1371983 words/s, in_qsize 20, out_qsize 0\n",
      "2019-05-11 10:53:26,838 : INFO : EPOCH 10 - PROGRESS: at 48.73% examples, 1379494 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:53:27,827 : INFO : EPOCH 10 - PROGRESS: at 53.54% examples, 1384916 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:53:28,848 : INFO : EPOCH 10 - PROGRESS: at 58.00% examples, 1374768 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:53:29,861 : INFO : EPOCH 10 - PROGRESS: at 63.03% examples, 1379599 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:53:30,855 : INFO : EPOCH 10 - PROGRESS: at 67.79% examples, 1377700 words/s, in_qsize 17, out_qsize 2\n",
      "2019-05-11 10:53:31,864 : INFO : EPOCH 10 - PROGRESS: at 72.49% examples, 1380801 words/s, in_qsize 18, out_qsize 1\n",
      "2019-05-11 10:53:32,875 : INFO : EPOCH 10 - PROGRESS: at 77.35% examples, 1386197 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:53:33,876 : INFO : EPOCH 10 - PROGRESS: at 81.72% examples, 1383695 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:53:34,884 : INFO : EPOCH 10 - PROGRESS: at 86.44% examples, 1385347 words/s, in_qsize 17, out_qsize 2\n",
      "2019-05-11 10:53:35,892 : INFO : EPOCH 10 - PROGRESS: at 91.72% examples, 1390077 words/s, in_qsize 19, out_qsize 0\n",
      "2019-05-11 10:53:36,895 : INFO : EPOCH 10 - PROGRESS: at 96.03% examples, 1384368 words/s, in_qsize 17, out_qsize 2\n",
      "2019-05-11 10:53:37,625 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-05-11 10:53:37,625 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-05-11 10:53:37,625 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-05-11 10:53:37,645 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-05-11 10:53:37,647 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-05-11 10:53:37,652 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-05-11 10:53:37,654 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-05-11 10:53:37,660 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-11 10:53:37,662 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-11 10:53:37,662 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-11 10:53:37,664 : INFO : EPOCH - 10 : training on 41519355 raw words (30350312 effective words) took 21.9s, 1387904 effective words/s\n",
      "2019-05-11 10:53:37,665 : INFO : training on a 415193550 raw words (303501244 effective words) took 218.4s, 1389944 effective words/s\n",
      "2019-05-11 10:53:37,665 : INFO : saving Word2Vec object under D:\\PyProjects\\_references_nb\\10_ml-stats-applications\\nlp-00-nlp-in-practice\\0.01_word2vec\\models\\hotel-reviews-148-01.model, separately None\n",
      "2019-05-11 10:53:37,666 : INFO : not storing attribute vectors_norm\n",
      "2019-05-11 10:53:37,667 : INFO : not storing attribute cum_table\n",
      "2019-05-11 10:53:39,666 : INFO : saved D:\\PyProjects\\_references_nb\\10_ml-stats-applications\\nlp-00-nlp-in-practice\\0.01_word2vec\\models\\hotel-reviews-148-01.model\n"
     ]
    }
   ],
   "source": [
    "vector_size = 152\n",
    "\n",
    "path_to_save_model = models_folder / f'hotel-reviews-{vector_size}-01.model'\n",
    "path_to_load_model = models_folder / f'hotel-reviews-{vector_size}-01.model'\n",
    "\n",
    "str_to_save_model = str(path_to_save_model.absolute())\n",
    "str_to_load_model = str(path_to_load_model.absolute())\n",
    "\n",
    "if create_new_model:\n",
    "    model = gensim.models.Word2Vec (documents, size=vector_size, window=10, min_count=2, workers=10)\n",
    "    model.train(documents,total_examples=len(documents),epochs=10)\n",
    "    model.save(str_to_save_model)\n",
    "else:\n",
    "    model = gensim.models.Word2Vec.load(str_to_load_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec has several components: the full model (just saved) but also the **KeyedVectors** accessible by `model.wv` and that is the matrix of all the trained word vectors (one set of weight for each word in vocabulary. In this case, this is 152 weight as the model was trained with `size=152`. `model.wv` is more compact than the full model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Each word such as <dirty> is represented by 148 weights. Example:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.33767405,  0.6794093 , -1.6494437 ,  2.4061284 , -3.0932152 ,\n",
       "       -0.97720116,  1.4268272 ,  0.7298138 , -2.1268995 , -0.5888922 ,\n",
       "       -4.146032  , -0.4465947 , -0.58673054,  1.1146352 ,  4.5479803 ,\n",
       "        1.5421354 , -1.0696585 , -0.83592707, -1.2704722 ,  4.144639  ,\n",
       "        1.8786309 , -3.0097766 ,  2.997035  , -2.4905162 ,  0.5071115 ,\n",
       "       -0.7466245 , -0.00872485, -0.8291812 , -1.1678741 ,  3.507877  ,\n",
       "       -0.12339821, -1.2952466 , -0.5034353 ,  0.31752926, -4.2167687 ,\n",
       "       -1.5882038 , -0.5982395 , -0.47037283,  3.591101  ,  3.8669853 ,\n",
       "       -2.799466  ,  2.625027  ,  0.141962  ,  0.29805347, -2.7182837 ,\n",
       "       -2.878581  , -0.5114344 ,  0.26465195, -3.2276485 , -4.5505657 ,\n",
       "        1.1102492 ,  3.065824  , -2.892303  ,  0.38926795, -1.639663  ,\n",
       "        3.3078532 ,  1.3884318 , -2.67059   ,  3.959185  ,  1.0809343 ,\n",
       "       -1.9843053 , -0.7903039 , -0.48452562, -3.772664  ,  1.5287273 ,\n",
       "       -1.0552201 , -3.2674336 ,  0.57016224,  3.9062545 ,  2.5715709 ,\n",
       "        1.4392201 , -1.6554326 , -0.77793795, -1.3020278 , -0.30339614,\n",
       "       -3.7467701 ,  3.1971653 , -4.0504937 ,  1.9420347 ,  2.1797345 ,\n",
       "       -1.1548145 ,  0.10476132,  0.6828318 , -0.6640708 , -1.8536729 ,\n",
       "       -1.7078456 , -1.0486208 , -1.9398694 , -0.45732486,  0.60742956,\n",
       "        0.33058393, -3.1809216 , -2.542124  , -0.592482  ,  2.495329  ,\n",
       "        2.1266007 , -2.8776345 ,  0.28634766, -0.05053165, -2.038669  ,\n",
       "        1.4565911 , -1.9381738 , -1.4898963 , -3.4856453 , -0.0088072 ,\n",
       "        0.84083766,  5.324309  ,  3.0053704 ,  3.1099296 , -0.68438005,\n",
       "       -2.6932583 ,  2.6174679 ,  2.171775  ,  3.2169461 , -1.5291162 ,\n",
       "       -0.04035499,  1.7786735 , -0.603788  ,  2.9982102 , -1.3050442 ,\n",
       "       -0.9231922 ,  2.9591467 , -1.2826083 ,  1.0832907 , -1.7749528 ,\n",
       "        1.7853067 , -3.5595686 , -1.9486382 ,  1.7859913 ,  0.8270931 ,\n",
       "       -3.9802482 ,  0.7797727 , -0.9992933 ,  1.2014449 ,  1.3832395 ,\n",
       "        3.250649  , -0.36400193, -1.4038029 , -4.8070054 , -0.07091992,\n",
       "        1.299971  , -1.9444194 , -4.310484  ,  1.5966594 , -2.946665  ,\n",
       "        3.4620295 ,  0.08128259,  4.0910573 ], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "word = 'dirty'\n",
    "keyedvectors = model.wv[word]\n",
    "print(f'Each word such as <{word}> is represented by {len(keyedvectors)} weights. Example:')\n",
    "display(keyedvectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, let's look at some output \n",
    "This first example shows a simple case of looking up words similar to the word `dirty`. All we need to do here is to call the `most_similar` function and provide the word `dirty` as the positive example. This returns the top 10 similar words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-11 10:54:12,597 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('filthy', 0.8638341426849365),\n",
       " ('unclean', 0.7788464426994324),\n",
       " ('stained', 0.7784373760223389),\n",
       " ('grubby', 0.7623147964477539),\n",
       " ('dusty', 0.760489284992218),\n",
       " ('smelly', 0.7517919540405273),\n",
       " ('dingy', 0.7328144311904907),\n",
       " ('disgusting', 0.721342146396637),\n",
       " ('mouldy', 0.7155332565307617),\n",
       " ('gross', 0.7071250081062317)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = \"dirty\"\n",
    "model.wv.most_similar (positive=w1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks pretty good, right? Let's look at a few more. Let's look at similarity for `polite`, `france` and `shocked`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('courteous', 0.9135578274726868),\n",
       " ('friendly', 0.8357058167457581),\n",
       " ('cordial', 0.7961171865463257),\n",
       " ('professional', 0.7880101203918457),\n",
       " ('curteous', 0.7746528387069702),\n",
       " ('attentive', 0.768876314163208)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look up top 6 words similar to 'polite'\n",
    "w1 = [\"polite\"]\n",
    "model.wv.most_similar (positive=w1, topn=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gaulle', 0.637433648109436),\n",
       " ('germany', 0.6352967023849487),\n",
       " ('canada', 0.6226556897163391),\n",
       " ('spain', 0.6202784776687622),\n",
       " ('mexico', 0.6196963787078857),\n",
       " ('england', 0.6120783090591431)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look up top 6 words similar to 'france'\n",
    "w1 = [\"france\"]\n",
    "model.wv.most_similar (positive=w1,topn=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('horrified', 0.8334214687347412),\n",
       " ('amazed', 0.79375159740448),\n",
       " ('astonished', 0.7687221169471741),\n",
       " ('stunned', 0.765560507774353),\n",
       " ('appalled', 0.7418582439422607),\n",
       " ('dismayed', 0.7278842926025391)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look up top 6 words similar to 'shocked'\n",
    "w1 = [\"shocked\"]\n",
    "model.wv.most_similar (positive=w1,topn=6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's, nice. You can even specify several positive examples to get things that are related in the provided context and provide negative examples to say what should not be considered as related. In the example below we are asking for all items that *relate to bed* only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mattress', 0.7131335735321045),\n",
       " ('duvet', 0.7086372971534729),\n",
       " ('blanket', 0.69327712059021),\n",
       " ('quilt', 0.6866929531097412),\n",
       " ('matress', 0.6786055564880371),\n",
       " ('pillowcase', 0.6777018308639526),\n",
       " ('sheets', 0.6306957006454468),\n",
       " ('quilts', 0.6306055784225464),\n",
       " ('pillows', 0.6297816038131714),\n",
       " ('foam', 0.6170405149459839)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get everything related to stuff on the bed\n",
    "w1 = [\"bed\",'sheet','pillow']\n",
    "w2 = ['couch']\n",
    "model.wv.most_similar (positive=w1,negative=w2,topn=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity between two words in the vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can even use the Word2Vec model to return the similarity between two words that are present in the vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7517919234077541"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similarity between two different words\n",
    "model.wv.similarity(w1=\"dirty\",w2=\"smelly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999999998"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similarity between two identical words\n",
    "model.wv.similarity(w1=\"dirty\",w2=\"dirty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2742473188979062"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similarity between two unrelated words\n",
    "model.wv.similarity(w1=\"dirty\",w2=\"clean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the hood, the above three snippets computes the cosine similarity between the two specified words using word vectors of each. From the scores, it makes sense that `dirty` is highly similar to `smelly` but `dirty` is dissimilar to `clean`. If you do a similarity between two identical words, the score will be 1.0 as the range of the cosine similarity score will always be between [0.0-1.0]. You can read more about cosine similarity scoring [here](https://en.wikipedia.org/wiki/Cosine_similarity)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the odd one out\n",
    "You can even use Word2Vec to find odd items given a list of items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'france'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which one is the odd one out in this list?\n",
    "model.wv.doesnt_match([\"cat\",\"dog\",\"france\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'shower'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which one is the odd one out in this list?\n",
    "model.wv.doesnt_match([\"bed\",\"pillow\",\"duvet\",\"shower\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding some of the parameters\n",
    "To train the model earlier, we had to set some parameters. Now, let's try to understand what some of them mean. For reference, this is the command that we used to train the model.\n",
    "\n",
    "```\n",
    "model = gensim.models.Word2Vec (documents, size=150, window=10, min_count=2, workers=10)\n",
    "```\n",
    "\n",
    "### `size`\n",
    "The size of the dense vector to represent each token or word. If you have very limited data, then size should be a much smaller value. If you have lots of data, its good to experiment with various sizes. A value of 100-150 has worked well for me. \n",
    "\n",
    "### `window`\n",
    "The maximum distance between the target word and its neighboring word. If your neighbor's position is greater than the maximum window width to the left and the right, then, some neighbors are not considered as being related to the target word. In theory, a smaller window should give you terms that are more related. If you have lots of data, then the window size should not matter too much, as long as its a decent sized window. \n",
    "\n",
    "### `min_count`\n",
    "Minimium frequency count of words. The model would ignore words that do not statisfy the `min_count`. Extremely infrequent words are usually unimportant, so its best to get rid of those. Unless your dataset is really tiny, this does not really affect the model.\n",
    "\n",
    "### `workers`\n",
    "How many threads to use behind the scenes?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When should you use Word2Vec?\n",
    "\n",
    "There are many application scenarios for Word2Vec. Imagine if you need to build a sentiment lexicon. Training a Word2Vec model on large amounts of user reviews helps you achieve that. You have a lexicon for not just sentiment, but for most words in the vocabulary. \n",
    "\n",
    "Beyond, raw unstructured text data, you could also use Word2Vec for more structured data. For example, if you had tags for a million stackoverflow questions and answers, you could find tags that are related to a given tag and recommend the related ones for exploration. You can do this by treating each set of co-occuring tags as a \"sentence\" and train a Word2Vec model on this data. Granted, you still need a large number of examples to make it work. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
